所谓的贝叶斯方法源于 托马斯·贝叶斯（Thomas Bayes）生前为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后才由他的一位朋友发表出来的。在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。这个问题，就是所谓的**逆概**问题。

实际上，贝叶斯当时的论文只是对这个问题的一个直接的求解尝试，并不清楚他当时是不是已经意识到这里面包含着的深刻的思想。然而后来，贝叶斯方法席卷了概率论，并将应用延伸到各个问题领域，所有需要作出概率预测的地方都可以见到贝叶斯方法的影子，特别地，贝叶斯是机器学习的核心方法之一。

这背后的深刻原因在于，现实世界本身就是不确定的，人类的观察能力是有局限性的（否则有很大一部分科学就没有必要做了——设想我们能够直接观察到电子的运行，还需要对原子模型争吵不休吗？），我们日常所观察到的只是事物表面上的结果，沿用刚才那个袋子里面取球的比方，我们往往只能知道从里面取出来的球是什么颜色，而并不能直接看到袋子里面实际的情况。这个时候，我们就需要提供一个**猜测**（hypothesis，更为严格的说法是“假设”，这里用“猜测”更通俗易懂一点），所谓猜测，当然就是不确定的（很可能有好多种乃至无数种猜测都能满足目前的观测），但也绝对不是两眼一抹黑瞎蒙——具体地说，我们需要做两件事情：

1. 算出各种不同猜测的可能性大小。计算特定猜测的**后验概率**，对于连续的猜测空间则是计算猜测的概率密度函数。
2. 算出最靠谱的猜测是什么。就是所谓的模型比较，模型比较如果不考虑先验概率的话就是最大似然方法。

# 贝叶斯公式

贝叶斯公式是怎么来的？我们使用 wikipedia 上的一个例子：

> 一所学校里面有 60% 的男生，40% 的女生。男生总是穿长裤，女生则一半穿长裤一半穿裙子。

有了这些信息之后我们可以容易地计算“随机选取一个学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。然而，假设你走在校园中，迎面走来一个穿长裤的学生（很不幸的是你高度近视，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是男生的概率是多大吗？

这里可以把问题重新叙述成：`你在校园里面随机游走，遇到了 N 个穿长裤的人（仍然假设你无法直接观察到他们的性别），问这 N 个人里面有多少个男生`。这样的话，问题就简单了许多，可以算出学校里面有多少穿长裤的，然后在这些人里面再算出有多少男生？

我们来算一算：假设学校里面人的总数是 U 个。60% 的男生都穿长裤，于是我们得到了 U * P(Boy) * P(Pants|Boy) 个穿长裤的男生。40% 的女生里面又有一半（50%）是穿长裤的，于是我们又得到了 U * P(Girl) * P(Pants|Girl) 个穿长裤的（女生）。加起来一共是 U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl) 个穿长裤的，其中有 U * P(Boy) * P(Pants|Boy) 个男生。这里：

* P(Boy) 是男生的概率 = 60%，这里可以简单的理解为男生的比例；
* P(Pants|Boy) 是**条件概率**，即在 Boy 这个条件下穿长裤的概率是多大，这里是 100% ，因为所有男生都穿长裤；

我们要求的是穿长裤的人里面有多少男生 P(Boy|Pants) ，这里计算的结果是 U * P(Boy) * P(Pants|Boy) / [U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)]。容易发现这里校园内人的总数是无关的，可以消去。于是得到

```
P(Boy|Pants) = P(Boy) * P(Pants|Boy) / [P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)]
```

注意，如果把上式收缩起来，分母其实就是 P(Pants) ，分子其实就是 P(Pants, Boy) 。而这个比例很自然地就读作：在穿长裤的人P(Pants)里面有多少穿长裤的男孩P(Pants, Boy)。上式中的 Pants 和 Boy/Girl 可以指代一切东西，所以其一般形式就是：

```
P(B|A) = P(A|B) * P(B) / [P(A|B) * P(B) + P(A|~B) * P(~B) ]
```

收缩起来就是：

```
P(B|A) = P(A ∩ B) / P(A)  
P(B|A) * P(A) = P(A ∩ B)    // 等价于上面
```

上面 P(A ∩ B) 表示事件 A 和 B 同时发生的概率，如下文氏图所示：

![][1]

## 公式的理解

我们可以将上面的公式进一步表达为如下格式：

```
P(A|B) = P(A) * P(B|A) / P(B)
```

这里 P(A) 称为**先验概率**（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。P(A|B)称为**后验概率**（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。P(B|A)/P(B)称为**可能性函数**（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

对上面的例子来说，

```
P(Boy|Pants) = P(Boy) * P(Pants|Boy) / P(Pants)
```

P(Boy) = 60% ，这是一个先验概率，即没有看到一个穿长裤的同学之前，任意一个同学是男生的概率。P(Pants) 也是一个先验概率，表示看到一个穿长裤同学的概率。问题是在看到一个穿长裤同学的的情况下，判断这个同学是男生的概率有多大，即求P(Boy|Pants)，我们把这个概率叫做**后验概率**，即在一个事件P(Pants)发生之后，对P(Boy)的修正。

# 更多阅读

[数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)  
[贝叶斯推断及其互联网应用（一）：定理简介](http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html)  
[贝叶斯推断及其互联网应用（二）：过滤垃圾邮件](http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_two.html)  
[How to Write a Spelling Corrector](http://norvig.com/spell-correct.html)  



[1]: http://7xrlu9.com1.z0.glb.clouddn.com/Math_BayesTheorem_1.jpg

